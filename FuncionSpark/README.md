# Demo DataProc Spark Cloud Functions
ğŸ’¡ Notas

- C4 Model para diagramas
- MÃ¡ximo 2 niveles de anidaciÃ³n para numeraciÃ³n de tÃ­tulos (i.e. 2.1.)
- 1 video por documentaciÃ³n. Si existen vÃ¡rios videos, unirlos en uno solo.
</aside>

# 1. Objetivo

Desplegar una arquitectura basada en eventos haciendo uso de los servicios: Cloud Functions, DataProc y Cloud Storage; el sistema deberÃ¡ de funcionar de la siguiente forma:

1. Una vez un nuevo archivo sea escrito correctamente al bucket: raw-data-spark la Cloud Function spark-function deberÃ¡ de ejecutar la plantilla de Dataproc: spark.
2. La plantilla de Dataproc deberÃ¡ de leer los datos ingresados al bucket, procesarlos y almacenarlos en el bucket: output-spark en formato Delta Lake.
3. Un â€œcheckpointâ€ de Spark deberÃ¡ de ser incluido con el fin de no reprocesar datos anteriormente procesados. 
4. Tanto el cÃ³digo usado por la plantilla de Dataproc, como el cÃ³digo usado por la Cloud Function deberÃ¡n de ser almacenados en un Bucket de Cloud Storage. 

# 2. Recursos usados

| Recurso | Tipo | Version | DescripciÃ³n |
| --- | --- | --- | --- |
| Cloud Function |  |  | Usado para ejecutar cierto codigo dado, una vez se crea un nuevo archivo en el raw-data-bucket. |
| Dataproc |  |  | Usado para procesar los datos presentes en el Bucket raw-data-spark y actualizar la tabla Delta presente en el Bucker output-spark. |
| Cloud Storage |  |  | Usado para almacenar en distintos Buckets el cÃ³digo, tanto de la funciÃ³n como de dataproc, los archivos a ser procesados por dataproc y la tabla Delta.  |

# 3. Ambiente de desarrollo

En esta secciÃ³n se proporciona una visiÃ³n general fundamental para el desarrollo del proyecto. En ella, se detalla la "Estructura del CÃ³digo", delineando la organizaciÃ³n jerÃ¡rquica de archivos y directorios que sustenta el proyecto.

## Estructura del cÃ³digo

A continuaciÃ³n, exploraremos la disposiciÃ³n de archivos y carpetas en nuestro proyecto. A continuaciÃ³n, se muestra una visiÃ³n general de cÃ³mo se organizan los archivos y las subcarpetas en relaciÃ³n con el directorio principal del proyecto.

```
DemoCloudFunctions-Dataproc&Spark
â”œâ”€â”€ data
â”‚Â Â  â””â”€â”€ clients3.csv
â”œâ”€â”€ dataproc
â”‚Â Â  â””â”€â”€ main.py
â”œâ”€â”€ deploy
â”‚Â Â  â”œâ”€â”€ makefile
â”‚Â Â  â””â”€â”€ spark.yaml
â”œâ”€â”€ docs
â”œâ”€â”€ function
â”‚Â Â  â”œâ”€â”€ main.py
â”‚Â Â  â””â”€â”€ requirements.txt
â””â”€â”€ generator
    â””â”€â”€ client_gener.py
```

# 4. Arquitectura de la soluciÃ³n

El siguiente diagrama detalla la arquitectura de la soluciÃ³n implementada:

![Diagrama de despliegue: Infraestructura de GCP usada.](https://dev.azure.com/quind/0d6ed6ae-137f-4fce-9c85-e34317a683a5/_apis/git/repositories/b0b783d7-94ec-4456-ac27-1d826d004e7c/items?path=/docs/arquiFuncionSpark.png&versionDescriptor%5BversionOptions%5D=0&versionDescriptor%5BversionType%5D=0&versionDescriptor%5Bversion%5D=main&resolveLfs=true&%24format=octetStream&api-version=5.0)

Diagrama de despliegue: Infraestructura de GCP usada.

# 5. ImplementaciÃ³n

## IAM y permisos requeridos

La siguiente tabla muestra los roles y permisos requeridos por Cloud Functions para funcionar correctamente.

| Nombre Rol | DescripciÃ³n |
| --- | --- |
| roles/iam.serviceAccountTokenCreator | Permite la creaciÃ³n tokens de acceso para cuentas de servicio, lo que permite el acceso a recursos especÃ­ficos de manera segura. |
| roles/eventarc.eventReceiver | Permisos para actuar como receptor de eventos en aplicaciones basadas en Cloud Run. Las cuentas de servicio o usuarios que tengan este rol pueden suscribirse a eventos y ejecutar servicios de Cloud Run que se activan cuando ocurren esos eventos. |
| roles/pubsub.publisher | Sirve para conceder permisos a usuarios o servicios que necesitan la capacidad de publicar mensajes en temas de Pub/Sub; ayuda a controlar y gestionar el flujo de informaciÃ³n del proyecto. |

## Requisitos previos

Para una correcta ejecucion, el proyecto requiere de:

1. Instalar la CLI de Google Cloud [ğŸŒ **Instala la CLI de gcloud**](https://cloud.google.com/sdk/docs/install?hl=es-419) 
2. Crear la cuenta de servicio respectiva con roles y permisos; por defecto Cloud Functions usara la cuenta de servicio de Compute Engine. 
3. Instalar el comando â€œmakeâ€ para la distribuciÃ³n o Sistema Operativo correspondiente.

## GuÃ­a de uso

La siguiente guÃ­a brindarÃ¡ los pasos a seguir para un correcto funcionamiento y ejecuciÃ³n.

1. Inicializar la CLI de gcloud
    
    ```bash
    gcloud init
    ```
    
2. Clonar el cÃ³digo requerido
    
    ```bash
    git clone https://dev.azure.com/quind/big-data/_git/DemoCloudFunctions-Dataproc-Spark
    ```
    
3. Dirigirnos a la carpeta â€œdeployâ€ del repositorio clonado anteriormente
    
    ```bash
    cd DemoCloudFunctions-Dataproc-Spark/deploy
    ```
    
4. Ejecutar el archivo â€œmakefileâ€
    
    ```bash
    make
    ```